# =============================================================================
# ðŸ† SHL AI ASSESSMENT - FLASK API (APPENDIX 2 COMPLIANT)
# =============================================================================
# Usage: python app.py

import os
import json
import re
from flask import Flask, request, jsonify
from flask_cors import CORS
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

app = Flask(__name__)
CORS(app) Â # Allow frontend to access API

# ---------------------------------------------------------
# 1. LOAD ENGINE (Your Optimized Logic)
# ---------------------------------------------------------
print("ðŸ”§ Initializing API...")
# Change this line in app.py to point to the data subfolder
DATA_PATH = "data/raw_assessments.json" Â # Ensure this file is present

# Load Data
# Tell Python to use the universal web standard encoding
with open(DATA_PATH, "r", encoding="utf-8") as f:
Â  Â  raw_data = json.load(f)

documents = []
for item in raw_data:
Â  Â  name = item.get('name', 'Unknown')
Â  Â  test_types = item.get('test_type', [])
Â  Â  
Â  Â  # STRICT FILTERING 
Â  Â  if "Pre-packaged Job Solutions" in test_types: continue
Â  Â  if "Solution" in name and "Individual" not in str(test_types): continue

Â  Â  # Create Document
Â  Â  page_content = f"Title: {name} | Type: {', '.join(test_types)} | Desc: {item.get('description', '')}"
Â  Â  
Â  Â  # Tokenizer Logic
Â  Â  text = page_content.lower()
Â  Â  tokens = re.findall(r"\b[a-z][a-z0-9]*(?:[.+#][a-z0-9]+)*\+*", text)
Â  Â  stopwords = {"hire", "hiring", "role", "junior", "senior", "developer", "engineer", "test", "assessment", "looking", "need"}
Â  Â  doc_tokens = set(t for t in tokens if t not in stopwords and len(t) > 1)
Â  Â  
Â  Â  metadata = {
Â  Â  Â  Â  "name": name,
Â  Â  Â  Â  "url": item.get("url"),
Â  Â  Â  Â  "adaptive_support": item.get("adaptive_support", "No"),
Â  Â  Â  Â  "description": item.get("description", ""),
Â  Â  Â  Â  "duration": item.get("duration"),
Â  Â  Â  Â  "remote_support": item.get("remote_support", "Yes"),
Â  Â  Â  Â  "test_type": test_types,
Â  Â  Â  Â  "doc_tokens": list(doc_tokens)
Â  Â  }
Â  Â  documents.append(Document(page_content=page_content, metadata=metadata))

# Build Vector Index
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_db = FAISS.from_documents(documents, embeddings)
print("âœ… API Ready & Index Built.")

# ---------------------------------------------------------
# 2. HELPER FUNCTIONS
# ---------------------------------------------------------
def clean_query(text):
Â  Â  fluff = ["i am hiring for", "looking to hire", "i need a", "we are looking for"]
Â  Â  cleaned = text.lower()
Â  Â  for phrase in fluff: cleaned = cleaned.replace(phrase, "")
Â  Â  return " ".join(cleaned.split())

def extract_tokens(text):
Â  Â  tokens = re.findall(r"\b[a-z][a-z0-9]*(?:[.+#][a-z0-9]+)*\+*", text.lower())
Â  Â  stopwords = {"hire", "hiring", "role", "junior", "senior", "developer", "engineer"}
Â  Â  return set(t for t in tokens if t not in stopwords and len(t) > 1)

# ---------------------------------------------------------
# 3. ENDPOINTS (Strict Appendix 2 Spec)
# ---------------------------------------------------------

@app.route('/health', methods=['GET'])
def health_check():
Â  Â  """
Â  Â  Ref: Appendix 2, Page 6 
Â  Â  """
Â  Â  return jsonify({"status": "healthy"}), 200

@app.route('/recommend', methods=['POST'])
def recommend():
Â  Â  """
Â  Â  Ref: Appendix 2, Page 6 
Â  Â  """
Â  Â  try:
Â  Â  Â  Â  data = request.get_json()
Â  Â  Â  Â  if not data or 'query' not in data:
Â  Â  Â  Â  Â  Â  return jsonify({"error": "Missing 'query' field"}), 400
Â  Â  Â  Â  
Â  Â  Â  Â  query = data['query']
Â  Â  Â  Â  
Â  Â  Â  Â  # Hybrid Search Logic
Â  Â  Â  Â  opt_query = clean_query(query)
Â  Â  Â  Â  query_skills = extract_tokens(query)
Â  Â  Â  Â  num_q_skills = max(len(query_skills), 1)

Â  Â  Â  Â  raw_results = vector_db.similarity_search_with_score(opt_query, k=30)
Â  Â  Â  Â  scored_candidates = []
Â  Â  Â  Â  
Â  Â  Â  Â  for doc, distance in raw_results:
Â  Â  Â  Â  Â  Â  v_score = 1 / (1 + distance)
Â  Â  Â  Â  Â  Â  doc_skill_set = set(doc.metadata.get('doc_tokens', []))
Â  Â  Â  Â  Â  Â  overlap = len(query_skills.intersection(doc_skill_set))
Â  Â  Â  Â  Â  Â  skill_bonus = min(1.0, overlap / num_q_skills)
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  final_score = (v_score * 0.7) + (skill_bonus * 0.3)
Â  Â  Â  Â  Â  Â  scored_candidates.append((final_score, doc))
Â  Â  Â  Â  
Â  Â  Â  Â  scored_candidates.sort(key=lambda x: x[0], reverse=True)
Â  Â  Â  Â  top_results = [d for _, d in scored_candidates][:10] # Max 10 

Â  Â  Â  Â  # Response Format 
Â  Â  Â  Â  response_list = []
Â  Â  Â  Â  for doc in top_results:
Â  Â  Â  Â  Â  Â  md = doc.metadata
Â  Â  Â  Â  Â  Â  response_list.append({
Â  Â  Â  Â  Â  Â  Â  Â  "url": md.get("url"),
Â  Â  Â  Â  Â  Â  Â  Â  "name": md.get("name"),
Â  Â  Â  Â  Â  Â  Â  Â  "adaptive_support": md.get("adaptive_support"), # CORRECTED
Â  Â  Â  Â  Â  Â  Â  Â  "description": md.get("description"),
Â  Â  Â  Â  Â  Â  Â  Â  "duration": md.get("duration"),
Â  Â  Â  Â  Â  Â  Â  Â  "remote_support": md.get("remote_support"), # CORRECTED
Â  Â  Â  Â  Â  Â  Â  Â  "test_type": md.get("test_type")
Â  Â  Â  Â  Â  Â  })

Â  Â  Â  Â  return jsonify({"recommended_assessments": response_list}), 200

Â  Â  except Exception as e:
Â  Â  Â  Â  return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
Â  Â  # Run on port 5000 (Gunicorn will run on 8000)
Â  Â  app.run(host='0.0.0.0', port=5000)